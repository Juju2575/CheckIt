{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as alea\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARTS &amp; CULTURE</td>\n",
       "      <td>Modeling Agencies Enabled Sexual Predators For...</td>\n",
       "      <td>In October 2017, Carolyn Kramer received a dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ARTS &amp; CULTURE</td>\n",
       "      <td>Actor Jeff Hiller Talks “Bright Colors And Bol...</td>\n",
       "      <td>This week I talked with actor Jeff Hiller abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ARTS &amp; CULTURE</td>\n",
       "      <td>New Yorker Cover Puts Trump 'In The Hole' Afte...</td>\n",
       "      <td>The New Yorker is taking on President Donald T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ARTS &amp; CULTURE</td>\n",
       "      <td>Man Surprises Girlfriend By Drawing Them In Di...</td>\n",
       "      <td>Kellen Hickey, a 26-year-old who lives in Huds...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ARTS &amp; CULTURE</td>\n",
       "      <td>This Artist Gives Renaissance-Style Sculptures...</td>\n",
       "      <td>There’s something about combining the traditio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         category                                              title  \\\n",
       "0  ARTS & CULTURE  Modeling Agencies Enabled Sexual Predators For...   \n",
       "1  ARTS & CULTURE  Actor Jeff Hiller Talks “Bright Colors And Bol...   \n",
       "2  ARTS & CULTURE  New Yorker Cover Puts Trump 'In The Hole' Afte...   \n",
       "3  ARTS & CULTURE  Man Surprises Girlfriend By Drawing Them In Di...   \n",
       "4  ARTS & CULTURE  This Artist Gives Renaissance-Style Sculptures...   \n",
       "\n",
       "                                                body  \n",
       "0  In October 2017, Carolyn Kramer received a dis...  \n",
       "1  This week I talked with actor Jeff Hiller abou...  \n",
       "2  The New Yorker is taking on President Donald T...  \n",
       "3  Kellen Hickey, a 26-year-old who lives in Huds...  \n",
       "4  There’s something about combining the traditio...  "
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df = pd.read_csv('./news-article-categories.csv')\n",
    "articles_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6872 entries, 0 to 6876\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   category  6872 non-null   object\n",
      " 1   title     6872 non-null   object\n",
      " 2   body      6872 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 472.8+ KB\n"
     ]
    }
   ],
   "source": [
    "articles_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARTS &amp; CULTURE</td>\n",
       "      <td>Modeling Agencies Enabled Sexual Predators For...</td>\n",
       "      <td>In October 2017, Carolyn Kramer received a dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ARTS &amp; CULTURE</td>\n",
       "      <td>Actor Jeff Hiller Talks “Bright Colors And Bol...</td>\n",
       "      <td>This week I talked with actor Jeff Hiller abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ARTS &amp; CULTURE</td>\n",
       "      <td>New Yorker Cover Puts Trump 'In The Hole' Afte...</td>\n",
       "      <td>The New Yorker is taking on President Donald T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ARTS &amp; CULTURE</td>\n",
       "      <td>Man Surprises Girlfriend By Drawing Them In Di...</td>\n",
       "      <td>Kellen Hickey, a 26-year-old who lives in Huds...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ARTS &amp; CULTURE</td>\n",
       "      <td>This Artist Gives Renaissance-Style Sculptures...</td>\n",
       "      <td>There’s something about combining the traditio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         category                                              title  \\\n",
       "0  ARTS & CULTURE  Modeling Agencies Enabled Sexual Predators For...   \n",
       "1  ARTS & CULTURE  Actor Jeff Hiller Talks “Bright Colors And Bol...   \n",
       "2  ARTS & CULTURE  New Yorker Cover Puts Trump 'In The Hole' Afte...   \n",
       "3  ARTS & CULTURE  Man Surprises Girlfriend By Drawing Them In Di...   \n",
       "4  ARTS & CULTURE  This Artist Gives Renaissance-Style Sculptures...   \n",
       "\n",
       "                                                body  \n",
       "0  In October 2017, Carolyn Kramer received a dis...  \n",
       "1  This week I talked with actor Jeff Hiller abou...  \n",
       "2  The New Yorker is taking on President Donald T...  \n",
       "3  Kellen Hickey, a 26-year-old who lives in Huds...  \n",
       "4  There’s something about combining the traditio...  "
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6872 entries, 0 to 6871\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   category  6872 non-null   object\n",
      " 1   title     6872 non-null   object\n",
      " 2   body      6872 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 161.2+ KB\n"
     ]
    }
   ],
   "source": [
    "articles_df = articles_df.reset_index().drop(columns=\"index\")\n",
    "articles_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = articles_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6872"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ARTS & CULTURE    1001\n",
       "BUSINESS           501\n",
       "ENTERTAINMENT      501\n",
       "ENVIRONMENT        501\n",
       "POLITICS           501\n",
       "RELIGION           501\n",
       "SPORTS             501\n",
       "TECH               501\n",
       "WOMEN              501\n",
       "EDUCATION          490\n",
       "COMEDY             376\n",
       "SCIENCE            350\n",
       "MEDIA              347\n",
       "CRIME              300\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df[\"category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\seni2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenating the articles titles and bodies\n",
    "articles_content = (articles_df['title'] + ' ' + articles_df['body']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading a set of English stopwords\n",
    "english_stopset_1 = set(stopwords.words('english')).union(\n",
    "                  {\"things\", \"that's\", \"there's\", \"something\", \"take\", \"don't\", \"don\", \"may\", \n",
    "                   \"set\", \"might\", \"says\", \"including\", \"lot\", \"much\", \"said\", \"know\", \"want\",\n",
    "                   \"good\", \"step\", \"often\", \"going\", \"thing\", \"things\", \"think\", \"you're\",\n",
    "                   \"back\", \"actually\", \"better\", \"look\", \"find\", \"right\", \"example\", \n",
    "                   \"verb\", \"verbs\", \"even\", \"could\", \"can\", \"get\", \"show\", \"new\",\n",
    "                   \"get\", \"first\", \"two\", \"really\", \"way\", \"say\", \"percent\", \"many\",\n",
    "                   \"see\", \"use\", \"word\", \"words\", \"us\", \"make\", \"need\", \"never\", \"love\",\n",
    "                   \"see\", \"life\", \"go\", \"day\", \"play\", \"group\", \"former\",\"according\", \"year\",\n",
    "                   \"feel\", \"family\", \"thought\", \"story\", \"kind\", \"great\", \"felt\",\n",
    "                   \"games\", \"city\", \"cities\", \"body\", \"night\", \"light\", \"girl\", \"friend\",\n",
    "                   \"real\", \"person\", \"people\", \"big\", \"long\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exactly', \"we're\", 'shown', 'thereupon', 'describe', 'whom', 'able', 'some', 'the', 'with', 'getting', 'specifying', 'else', 'into', 'somewhere', 'last', 'truly', 'for', 'research', \"she'd\", 'inc.', 'sufficiently', 'name', 'including', 'us', \"i'll\", 'nearly', 'noone', 'my', 'definitely', 'quite', 'between', 'here', 'containing', 'twelve', 'step', 'entirely', 'whither', 'couldn', 'what', 'little', 'towards', 'fire', 'several', 'following', 'promptly', 'especially', \"there're\", 'found', 'made', 'arise', 'less', 'means', 'in', 'cant', 'placed', 'howbeit', 'd', 'were', 'who', 'beginnings', 'person', 'thus', 'minus', 'hopefully', 'whats', 'hes', 'past', 'cannot', \"hasn't\", 'refs', \"doesn't\", 'not', 'directly', 'r', 'neverless', 'welcome', 'considering', 'tell', 'oh', 'now', 'keep', 'self', 'its', 'top', 'ought', 'become', 'far', 'significant', 'fewer', \"who'll\", 'thoroughly', 'beforehand', 'should', 'each', 'sometimes', 'say', 'shed', 'when', 'insofar', 'though', 'doesn', \"mustn't\", 'at', 'does', 'ca', 's', 'furthermore', 'id', 'bottom', \"i'd\", 'announce', 'thereto', 'right', 'useful', 'part', 'mill', 'sincere', 'am', 'near', 'won', 'upwards', 'so', 'well', 'whence', 'nay', 'became', 'mrs', 'ord', 'namely', 'biol', 'ever', 'kind', 'full', 'mostly', 'thanx', 'seeming', 'proud', 'percent', 'which', 'throug', \"mightn't\", \"they've\", 'latterly', 'wheres', 'lot', 'next', 'sometime', 'never', 'fairly', 'ones', 'cry', 'them', 'let', 'too', \"here's\", 'ma', 'make', 'it', 'herein', 'could', 'thence', 'begins', 'adopted', 'more', 're', 'without', 'poorly', 'mainly', 'if', 'mug', 'potentially', 'formerly', 'anything', \"she's\", 'whereupon', 'thou', 'underneath', 'nevertheless', 'qv', 'story', 'adj', 'away', 'knows', 'within', 'twenty', 'im', \"wouldn't\", 'beginning', 'resulted', 'show', 'regards', 'sure', 'also', 'present', 'against', \"shouldn't\", 'indeed', \"he'll\", \"wasn't\", 'amidst', 'appreciate', \"that've\", 'thorough', 'always', 'thoughh', 'being', 'neither', 'anyway', 'usefully', 'backwards', 'across', 'later', 'owing', \"couldn't\", 'take', 'thing', 'ref', 'something', 'km', 'fifth', 'corresponding', 'thanks', 'hers', 'via', 'a', 'ahead', 'eleven', 'toward', 'ff', 'll', 'necessary', 'theyd', 'have', 'just', 'p', 'significantly', 'nor', 'verbs', 'hither', 'yourself', 'to', 'ltd', 'help', 'yes', 'auth', 'shouldn', 'there', 'nothing', 'thereof', 'an', 'already', 'necessarily', 'farther', 'six', 'shan', 'yourselves', 'specified', 'off', 'ok', 'nd', \"we'd\", 'why', 'widely', 'instead', 'c', 'weren', 'none', \"who'd\", 'con', 'move', 'once', \"can't\", 'this', 'youd', 'do', 'rather', 'caption', 'only', 'merely', 'before', \"there's\", \"what've\", 'state', 'set', 'two', 'versus', \"when's\", \"they'd\", 'comes', 'year', 'many', 'least', 'seen', 'lets', 'provided', 'friend', 'et-al', 'says', 'about', 'certainly', \"shan't\", 'ask', \"it's\", 'tends', 'hasnt', 'gave', \"that'll\", 'ago', 'brief', 'related', 'half', 'clearly', 'done', 'aside', 'liked', 'said', 'together', 'whenever', 'needn', 'opposite', 'except', 'beyond', 'til', 'looking', 'hello', 'where', 'added', 'affected', 'long', 'other', 'novel', 'myself', 'co', 'accordance', 'fix', 'ed', 'backward', 'downwards', 'thered', 'various', 'x', 'possible', 'go', 'gone', 'described', 'whos', \"oughtn't\", 'whether', 'appropriate', 'enough', 'substantially', 'see', 'example', 'did', 'night', 'ain', 'apparently', 'good', 'unto', 'thirty', 'trying', 'even', 'first', 'sixty', 'then', 'still', 'available', 'thin', 'sec', 'body', 'lower', 'lest', 'him', 'selves', 'that', 'no-one', 'vol', 'ignored', 'ts', 'around', 'sub', 'few', \"isn't\", 'j', 'big', 'currently', 'different', 'whilst', 'took', 'particularly', 'wants', 'ie', 'verb', 'way', 'seeing', 'be', 'begin', 'those', 'new', 'second', 'fifteen', 'apart', 'elsewhere', 'somewhat', 'hadn', 'somehow', 'below', 'going', 'lately', 'amount', 'e', 'amongst', 'overall', 'nine', 'saying', 'somethan', 'sent', 'whim', 'likewise', 'plus', 'f', 'given', 'been', 'rd', 'almost', 'regardless', 'couldnt', 'another', 'provides', 'thereby', 'third', 'love', 'l', \"it'd\", 'forty', 'whole', 'inner', 't', 'feel', 'miss', 'words', 'nobody', 'fifty', 'unfortunately', 'taking', 'previously', 'largely', 'believe', 'inasmuch', 'often', 'anyhow', 'one', 'ours', 'wed', 'similar', 'successfully', 'affects', 'omitted', 'wasn', 'sensible', 'mustn', 'soon', 'zero', 'tip', 'regarding', 'y', 'outside', 'shes', 'associated', 'had', 'whomever', \"mayn't\", 'ups', 'don', 'becomes', 'got', 'usually', 'results', 'm', \"you're\", 'as', 'computer', 'readily', \"you'll\", 'hereby', 'low', 'her', 'themselves', 'bill', 'neverf', 'per', 'abst', 'me', 'secondly', 'obtain', 'immediately', 'theyre', 'due', 'relatively', \"a's\", 'much', 'nowhere', 'side', 'wonder', 'ending', \"needn't\", 'having', 'group', 'yet', 'during', 'besides', 'round', \"you'd\", \"how's\", 'g', 'detail', 'et', 'front', 'known', 'sup', 'thru', 'kept', 'has', 'great', 'forever', 'along', 'stop', 'million', 'doing', 'information', 'life', 'forward', 'up', 'but', 'followed', \"i've\", 'will', 'anyways', 'wherein', 'accordingly', 'viz', \"there've\", 'from', 'best', 'system', 'states', 'beside', 'v', \"where's\", 'hardly', 'gives', 'everything', 'himself', 'b', 'eight', 'things', 'girl', 'slightly', 'of', 'among', 'myse', 'q', 'quickly', 'z', 'look', 'section', 'down', 'ex', 'further', 'keeps', 'needs', 'using', 'whoever', 'www', 'ah', \"we've\", 'I', 'anymore', 'maybe', 'concerning', 'inside', \"'ll\", 'possibly', 'use', 'uucp', 'cause', 'herse', \"should've\", 'thereafter', \"he'd\", 'page', 'latter', 'hereupon', 'eighty', 'unlikely', 'however', 'aren', \"ain't\", 'normally', 'okay', 'likely', 'himse', 'act', 'importance', 'you', 'dare', 'k', 'five', 'seven', 'ml', \"there'd\", 'probably', 'light', 'afterwards', 'vs', 'mr', 'throughout', 'w', 'abroad', 'ten', 'predominantly', 'since', 'reasonably', 'yours', 'asking', 'according', 'everybody', 'otherwise', 'again', 'mine', 'contains', \"t's\", 'no', 'twice', 'suggest', \"we'll\", \"what'll\", 'invention', 'appear', \"they're\", 'becoming', 'thousand', 'evermore', \"they'll\", 'everyone', 'changes', 'willing', 'seems', 'index', 'tries', 'important', 'consider', \"that's\", 'herself', 'taken', 'pages', 'back', 'better', 'unless', 'former', 'somebody', 'used', 'whereby', 'hereafter', 'obtained', 'every', 'while', 've', 'amid', 'saw', 'course', \"you've\", 'try', 'interest', 'resulting', 'games', 'family', \"there'll\", 'indicate', 'give', 'such', 'looks', 'inc', 'follows', 'onto', 'wherever', 'although', 'etc', 'therefore', 'keys', 'heres', 'either', 'behind', 'alone', 'makes', 'upon', 'immediate', 'by', 'he', 'want', 'above', 'city', 'certain', 'eg', 'thought', 'come', 'therein', 'end', 'similarly', 'happens', 'gotten', 'play', 'arent', 'people', 'four', 'whereafter', 'date', \"he's\", 'uses', 'out', 'ninety', 'h', 'actually', 'their', 'know', 'meantime', 'any', 'call', 'awfully', \"weren't\", 'our', 'perhaps', 'contain', \"it'll\", 'o', 'theres', 'strongly', 'theirs', 'causes', 'un', 'i', 'own', 'we', 'inward', 'vols', 'primarily', 'allows', 'specifically', 'presumably', 'someday', 'serious', 'isn', 'most', 'nos', \"let's\", 'old', 'may', 'showns', 'didn', 'wish', 'both', 'giving', 'pp', 'th', 'specify', 'seem', 'she', 'thank', 'came', \"c'mon\", 'all', 'or', 'someone', 'was', 'whose', 'would', 'mg', 'ran', 'real', 'consequently', 'recently', 'sorry', 'cities', 'went', 'mightn', 'recent', 'line', 'day', 'obviously', 'seemed', 'notwithstanding', 'usefulness', \"why's\", 'briefly', 'itself', 'till', 'gets', 'get', 'word', 'really', 'allow', \"i'm\", 'anyone', 'particular', 'despite', 'itd', 'thick', 'is', \"daren't\", \"haven't\", 'under', 'haven', 'whichever', 'others', \"aren't\", 'felt', 'na', 'anybody', 'goes', 'showed', \"c's\", 'because', \"'ve\", 'on', \"one's\", 'u', 'home', 'might', 'hi', 'your', 'think', \"hadn't\", 'ourselves', 'tried', 'value', 'than', 'unlike', 'co.', 'please', 'hid', \"she'll\", 'nonetheless', 'alongside', 'until', 'undoing', 'his', 'effect', 'they', 'hence', 'anywhere', 'must', 'whereas', 'empty', 'non', 'affecting', \"won't\", 'meanwhile', 'approximately', 'respectively', 'can', 'que', 'everywhere', 'need', 'same', 'put', 'wouldn', 'whatever', 'indicated', 'like', 'thats', 'after', 'through', 'com', \"didn't\", 'kg', 'edu', 'shows', 'itse', 'noted', 'therere', 'find', 'these', 'shall', 'world', 'seriously', 'and', 'de', 'forth', 'how', 'hundred', \"who's\", 'fill', 'n', 'are', 'whod', 'run', 'over', 'three', 'greetings', 'mean', 'moreover', 'indicates', 'hasn', 'very', \"what's\", \"don't\"}\n"
     ]
    }
   ],
   "source": [
    "# Importing a larger list of stopwords\n",
    "with open(\"./eng_stopwords.txt\", \"r\") as f:\n",
    "    eng_stopwords = f.readlines()\n",
    "    #print(eng_stopwords)\n",
    "\n",
    "for i in range(len(eng_stopwords)):\n",
    "    eng_stopwords[i] = eng_stopwords[i][:-1]\n",
    "\n",
    "english_stopset_2 = set(eng_stopwords).union(english_stopset_1)\n",
    "print(english_stopset_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing words of articles\n",
    "tokenizer = RegexpTokenizer(r\"(?u)[\\b\\#a-zA-Z][\\w&-_]+\\b\")\n",
    "articles_tokens = list(map(lambda d: [token for token in tokenizer.tokenize(d.lower()) if token not in english_stopset_2], articles_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing bigrams from unigrams (sets of two works frequently together in the corpus)\n",
    "bigram_transformer = models.Phrases(articles_tokens)\n",
    "articles_unigrams_bigrams_tokens = list(bigram_transformer[articles_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('women', 5455), ('trump', 4857), ('students', 3141), ('company', 3013), ('school', 2683), ('u.s', 2435), ('children', 2076), ('president', 2041), ('country', 2002), ('art', 1980), ('public', 1870), ('american', 1853), ('support', 1821), ('place', 1808), ('education', 1781), ('change', 1747), ('schools', 1728), ('woman', 1713), ('times', 1683), ('men', 1667), ('man', 1644), ('today', 1625), ('facebook', 1605), ('case', 1584), ('york', 1581), ('wrote', 1557), ('statement', 1548), ('making', 1528), ('power', 1514), ('asked', 1506), ('report', 1498), ('america', 1459), ('book', 1422), ('news', 1422), ('working', 1409), ('team', 1406), ('fact', 1399), ('history', 1373), ('community', 1372), ('white', 1336), ('experience', 1325), ('future', 1321), ('money', 1317), ('left', 1303), ('read', 1293), ('live', 1290), ('days', 1285), ('united', 1279), ('teachers', 1275), ('political', 1273), ('business', 1271), ('national', 1270), ('point', 1263), ('video', 1254), ('program', 1251), ('data', 1251), ('job', 1240), ('number', 1232), ('police', 1199), ('young', 1178), ('companies', 1173), ('parents', 1164), ('reported', 1162), ('kids', 1157), ('idea', 1150), ('month', 1140), ('god', 1137), ('black', 1118), ('face', 1103), ('law', 1103), ('government', 1101), ('talk', 1100), ('wanted', 1099), ('issue', 1076), ('question', 1068), ('process', 1067), ('hope', 1052), ('artists', 1050), ('matter', 1044), ('friends', 1037), ('office', 1030), ('understand', 1030), ('series', 1018), ('human', 1017), ('post', 1016), ('film', 1006), ('stories', 1003), ('start', 1002), ('problem', 1001), ('ways', 997), ('tuesday', 997), ('clear', 993), ('started', 984), ('project', 983), ('members', 981), ('months', 976), ('music', 976), ('university', 973), ('based', 972), ('artist', 971)]\n"
     ]
    }
   ],
   "source": [
    "#### Eventually use pattern librairy\n",
    "\n",
    "#Creating a dictionary and filtering out too rare and too common tokens\n",
    "english_dictionary = corpora.Dictionary(articles_unigrams_bigrams_tokens)\n",
    "english_dictionary.filter_extremes(no_below=5, no_above=0.2, keep_n=None)\n",
    "english_dictionary.compactify()\n",
    "print(english_dictionary.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing Bag-of-Words (BoW) for each article\n",
    "articles_bow = [english_dictionary.doc2bow(doc) for doc in articles_unigrams_bigrams_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the LDA topic model on English articles\n",
    "lda_model = models.LdaModel(articles_bow, id2word=english_dictionary, num_topics=24, passes=15, iterations=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing the topics for each article\n",
    "articles_lda = lda_model[articles_bow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.interfaces.TransformedCorpus object at 0x000001A662F01780>\n"
     ]
    }
   ],
   "source": [
    "print(articles_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics_top_words(model, max_words):\n",
    "    all_topics = model.show_topics(-1, max_words*2, False, False)\n",
    "    topics = []\n",
    "    for topic in all_topics:    \n",
    "        min_score_word = float(abs(topic[1][0][1])) / 10.\n",
    "        top_positive_words = list(map(lambda y: y[0].replace('_',' '), filter(lambda x: x[1] > min_score_word, topic[1])))[0:max_words]\n",
    "        topics.append('[' + ', '.join(top_positive_words) + ']')\n",
    "    return topics\n",
    "\n",
    "#Computing the main topic of each article\n",
    "topics_top_words = get_topics_top_words(lda_model, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[trump, president, god, america, political, do...</td>\n",
       "      <td>856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[children, friends, talk, kids, job, experienc...</td>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[art, artists, artist, space, human, works, pl...</td>\n",
       "      <td>773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[film, season, movie, music, book, series, song]</td>\n",
       "      <td>672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[students, education, school, schools, childre...</td>\n",
       "      <td>493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[water, scientists, climate change, science, e...</td>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[apple, data, technology, google, company, app...</td>\n",
       "      <td>407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[police, man, car, year-old, death, killed, bl...</td>\n",
       "      <td>366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[abuse, sexual assault, statement, allegations...</td>\n",
       "      <td>355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[game, team, event, players, win, sports, record]</td>\n",
       "      <td>291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[company, companies, billion, amazon, workers,...</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[facebook, social media, twitter, news, video,...</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[women, men, woman]</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[u.s, uber, company, fbi, russia, investigatio...</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[case, law, court, cases, legal, usa gymnastic...</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[money, cohen, pay, government, paid, payment,...</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[vote, campaign, democrats, protest, house, co...</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[post shared, samsung, abortion, abortions, an...</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[jewish, war, white, muslims, history, jews, p...</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[weinstein, security, fox news, white house, d...</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[giuliani, shooting, las vegas, hotel, gun, vi...</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[teachers, teacher, school, students, teaching...</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[vatican, pope, francis, pope francis, rocket,...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[texas, public schools, olympics, board, jacks...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                topic  count\n",
       "23  [trump, president, god, america, political, do...    856\n",
       "22  [children, friends, talk, kids, job, experienc...    785\n",
       "21  [art, artists, artist, space, human, works, pl...    773\n",
       "20   [film, season, movie, music, book, series, song]    672\n",
       "19  [students, education, school, schools, childre...    493\n",
       "18  [water, scientists, climate change, science, e...    409\n",
       "17  [apple, data, technology, google, company, app...    407\n",
       "16  [police, man, car, year-old, death, killed, bl...    366\n",
       "15  [abuse, sexual assault, statement, allegations...    355\n",
       "14  [game, team, event, players, win, sports, record]    291\n",
       "13  [company, companies, billion, amazon, workers,...    265\n",
       "12  [facebook, social media, twitter, news, video,...    178\n",
       "11                                [women, men, woman]    156\n",
       "10  [u.s, uber, company, fbi, russia, investigatio...    126\n",
       "9   [case, law, court, cases, legal, usa gymnastic...    121\n",
       "8   [money, cohen, pay, government, paid, payment,...    119\n",
       "7   [vote, campaign, democrats, protest, house, co...    104\n",
       "6   [post shared, samsung, abortion, abortions, an...     96\n",
       "5   [jewish, war, white, muslims, history, jews, p...     88\n",
       "4   [weinstein, security, fox news, white house, d...     73\n",
       "3   [giuliani, shooting, las vegas, hotel, gun, vi...     71\n",
       "2   [teachers, teacher, school, students, teaching...     33\n",
       "1   [vatican, pope, francis, pope francis, rocket,...     20\n",
       "0   [texas, public schools, olympics, board, jacks...     15"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_main_topics(corpus_lda, topics_labels):\n",
    "    min_strength = (1.0 / float(len(topics_labels)))/10 + 0.01\n",
    "    main_topics = map(lambda ts: sorted(ts, key=lambda t: -t[1])[0][0] if sorted(ts, key=lambda t: -t[1])[0][1] > min_strength else None, corpus_lda)\n",
    "    second_topics = map(lambda ts: sorted(ts, key=lambda t: -t[1])[1][0] if len(sorted(ts, key=lambda t: -t[1])) > 1 else None, corpus_lda)\n",
    "    main_topics_labels = map(lambda x: topics_labels[x] if x != None else '', main_topics)\n",
    "    second_topics_labels = map(lambda x: topics_labels[x] if x != None else '', second_topics)\n",
    "    return list(main_topics_labels), list(second_topics_labels)\n",
    "\n",
    "#Return the discovered topics, sorted by popularity\n",
    "corpus_main_topics, corpus_second_topics = get_main_topics(articles_lda, topics_top_words)\n",
    "\n",
    "main_topics_df = pd.DataFrame(corpus_main_topics, columns=['topic']).groupby('topic').size().sort_values(ascending=True).reset_index()\n",
    "main_topics_df.columns = ['topic','count']\n",
    "main_topics_df.sort_values('count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "[apple, data, technology, google, company, app, iphone]\n",
      "[jewish, war, white, muslims, history, jews, painting]\n",
      "[case, law, court, cases, legal, usa gymnastics, judge]\n",
      "[teachers, teacher, school, students, teaching, districts, gun violence]\n",
      "[water, scientists, climate change, science, energy, earth, researchers]\n",
      "[texas, public schools, olympics, board, jackson, bitcoin, gates]\n",
      "[abuse, sexual assault, statement, allegations, survivors, wrote, women]\n",
      "[post shared, samsung, abortion, abortions, animals, planned parenthood, christians]\n",
      "[vote, campaign, democrats, protest, house, congress, legislation]\n",
      "[police, man, car, year-old, death, killed, black]\n",
      "[trump, president, god, america, political, donald trump, election]\n",
      "[students, education, school, schools, children, support, community]\n",
      "[facebook, social media, twitter, news, video, company, content]\n",
      "[company, companies, billion, amazon, workers, business, pay]\n",
      "[u.s, uber, company, fbi, russia, investigation, statement]\n",
      "[game, team, event, players, win, sports, record]\n",
      "[film, season, movie, music, book, series, song]\n",
      "[children, friends, talk, kids, job, experience, wanted]\n",
      "[weinstein, security, fox news, white house, devos, yahoo, apple's]\n",
      "[money, cohen, pay, government, paid, payment, lawyer]\n",
      "[art, artists, artist, space, human, works, place]\n",
      "[vatican, pope, francis, pope francis, rocket, campbell, drone]\n",
      "[giuliani, shooting, las vegas, hotel, gun, video, room]\n",
      "[women, men, woman]\n"
     ]
    }
   ],
   "source": [
    "print(len(topics_top_words))\n",
    "\n",
    "for t in topics_top_words:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[film, season, movie, music, book, series, song]\n",
      "[trump, president, god, america, political, donald trump, election]\n",
      "16\n",
      "10\n",
      "[(6, 0.02718419), (10, 0.117340475), (16, 0.8485727)]\n"
     ]
    }
   ],
   "source": [
    "cmt1 = corpus_main_topics[1]\n",
    "cst1 = corpus_second_topics[1]\n",
    "print(cmt1)\n",
    "print(cst1)\n",
    "i_cmt1 = topics_top_words.index(cmt1)\n",
    "print(i_cmt1)\n",
    "i_cst1 = topics_top_words.index(cst1)\n",
    "print(i_cst1)\n",
    "print(articles_lda[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARTS & CULTURE\n",
      "Midcentury in Paris: A Visual Map of Modern Artists\n",
      "From the winding streets of Montmarte to cabaret-canvassed Pigalle, many of the world's most renowned talents inhabited the French capital. Here, an essential who's-who of the early Paris art scene.\n",
      "\n",
      " Much like the 1970s punk rock scene belonged to New York and Los Angeles was the birthplace of 1950s film noir, no locale is more synonymous with Modernism than Paris. Aspiring and established artists alike flocked from around the world to access the creative energy, visual innovation and flouris\n",
      "\n",
      "topic group 20\n",
      "[art, artists, artist, space, human, works, place]\n",
      "topic group 1\n",
      "[jewish, war, white, muslims, history, jews, painting]\n",
      "scores :\n",
      "[(20, 0.48487017), (1, 0.19475868), (11, 0.09529906), (0, 0.06722766), (12, 0.059935607), (15, 0.05215109), (4, 0.029675616), (5, 0.010967277)]\n"
     ]
    }
   ],
   "source": [
    "n = alea.randint(articles_df.shape[0])\n",
    "#n = 1\n",
    "article = articles_df.iloc[n]\n",
    "print(article['category'])\n",
    "print(article['title'])\n",
    "print(article['body'][:500])\n",
    "print()\n",
    "\n",
    "main_topics = corpus_main_topics[n]\n",
    "second_topics = corpus_second_topics[n]\n",
    "print(\"topic group \" + str(topics_top_words.index(corpus_main_topics[n])))\n",
    "print(main_topics)\n",
    "print(\"topic group \" + str(topics_top_words.index(corpus_second_topics[n])))\n",
    "print(second_topics)\n",
    "print(\"scores :\")\n",
    "print(sorted(articles_lda[n], key=lambda t: -t[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ARTS & CULTURE\n",
      "1 BUSINESS\n",
      "2 COMEDY\n",
      "3 CRIME\n",
      "4 EDUCATION\n",
      "5 ENTERTAINMENT\n",
      "6 ENVIRONMENT\n",
      "7 MEDIA\n",
      "8 POLITICS\n",
      "9 RELIGION\n",
      "10 SCIENCE\n",
      "11 SPORTS\n",
      "12 TECH\n",
      "13 WOMEN\n"
     ]
    }
   ],
   "source": [
    "categories = articles_df[\"category\"].unique()\n",
    "for i in range(len(categories)):\n",
    "    print(str(i) + ' ' + categories[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "CategoryLinks = [12, 9, 3, 4, 6, 5, 13, 2, 8, 3, 8, 4, 7, 1, 8, 11, 0, 5, 2, 8, 0, 9, 3, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [apple, data, technology, google, company, app, iphone]\n",
      "1 [jewish, war, white, muslims, history, jews, painting]\n",
      "2 [case, law, court, cases, legal, usa gymnastics, judge]\n",
      "3 [teachers, teacher, school, students, teaching, districts, gun violence]\n",
      "4 [water, scientists, climate change, science, energy, earth, researchers]\n",
      "5 [texas, public schools, olympics, board, jackson, bitcoin, gates]\n",
      "6 [abuse, sexual assault, statement, allegations, survivors, wrote, women]\n",
      "7 [post shared, samsung, abortion, abortions, animals, planned parenthood, christians]\n",
      "8 [vote, campaign, democrats, protest, house, congress, legislation]\n",
      "9 [police, man, car, year-old, death, killed, black]\n",
      "10 [trump, president, god, america, political, donald trump, election]\n",
      "11 [students, education, school, schools, children, support, community]\n",
      "12 [facebook, social media, twitter, news, video, company, content]\n",
      "13 [company, companies, billion, amazon, workers, business, pay]\n",
      "14 [u.s, uber, company, fbi, russia, investigation, statement]\n",
      "15 [game, team, event, players, win, sports, record]\n",
      "16 [film, season, movie, music, book, series, song]\n",
      "17 [children, friends, talk, kids, job, experience, wanted]\n",
      "18 [weinstein, security, fox news, white house, devos, yahoo, apple's]\n",
      "19 [money, cohen, pay, government, paid, payment, lawyer]\n",
      "20 [art, artists, artist, space, human, works, place]\n",
      "21 [vatican, pope, francis, pope francis, rocket, campbell, drone]\n",
      "22 [giuliani, shooting, las vegas, hotel, gun, video, room]\n",
      "23 [women, men, woman]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(topics_top_words)):\n",
    "    print(str(i) + ' ' + topics_top_words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6872\n",
      "['WOMEN', 'ARTS & CULTURE', 'WOMEN', 'ARTS & CULTURE', 'ARTS & CULTURE', 'ARTS & CULTURE', 'WOMEN', 'SPORTS', 'ARTS & CULTURE', 'ARTS & CULTURE', 'ARTS & CULTURE', 'ARTS & CULTURE', 'POLITICS', 'RELIGION', 'ARTS & CULTURE', 'ARTS & CULTURE', 'ARTS & CULTURE', 'ARTS & CULTURE', 'BUSINESS', 'WOMEN', 'POLITICS', 'RELIGION', 'ARTS & CULTURE', 'ARTS & CULTURE', 'ENVIRONMENT', 'ARTS & CULTURE', 'ENVIRONMENT', 'ARTS & CULTURE', 'WOMEN', 'ARTS & CULTURE', 'ARTS & CULTURE', 'ARTS & CULTURE', 'WOMEN', 'ARTS & CULTURE', 'CRIME', 'SPORTS', 'ARTS & CULTURE', 'RELIGION', 'WOMEN', 'ARTS & CULTURE', 'POLITICS', 'WOMEN', 'ARTS & CULTURE', 'ARTS & CULTURE', 'SPORTS', 'EDUCATION', 'ENTERTAINMENT', 'RELIGION', 'ARTS & CULTURE', 'RELIGION', 'ARTS & CULTURE', 'ARTS & CULTURE', 'ARTS & CULTURE', 'ARTS & CULTURE', 'RELIGION', 'ARTS & CULTURE', 'COMEDY', 'EDUCATION', 'POLITICS', 'ARTS & CULTURE', 'ARTS & CULTURE', 'ENTERTAINMENT', 'ARTS & CULTURE', 'ARTS & CULTURE', 'ARTS & CULTURE', 'COMEDY', 'EDUCATION', 'ARTS & CULTURE', 'ENTERTAINMENT', 'COMEDY', 'ENTERTAINMENT', 'RELIGION', 'POLITICS', 'ENVIRONMENT', 'ARTS & CULTURE', 'ARTS & CULTURE', 'ARTS & CULTURE', 'RELIGION', 'BUSINESS', 'ARTS & CULTURE', 'ARTS & CULTURE', 'ARTS & CULTURE', 'ARTS & CULTURE', 'ARTS & CULTURE', 'ENVIRONMENT', 'RELIGION', 'ARTS & CULTURE', 'ARTS & CULTURE', 'ARTS & CULTURE', 'POLITICS', 'POLITICS', 'POLITICS', 'POLITICS', 'ARTS & CULTURE', 'ARTS & CULTURE', 'ARTS & CULTURE', 'SPORTS', 'ARTS & CULTURE', 'MEDIA', 'ARTS & CULTURE']\n"
     ]
    }
   ],
   "source": [
    "corpus_lda_category_index = [topics_top_words.index(topic) for topic in corpus_main_topics]\n",
    "corpus_lda_category_topics = [str(topic) for topic in corpus_main_topics]\n",
    "\n",
    "corpus_converted_category = [categories[CategoryLinks[i]] for i in corpus_lda_category_index]\n",
    "print(len(corpus_converted_category))\n",
    "print(corpus_converted_category[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6872, 3)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6872 entries, 0 to 6871\n",
      "Data columns (total 3 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   computed_category    6872 non-null   object\n",
      " 1   lda_category_index   6872 non-null   int64 \n",
      " 2   lda_category_topics  6872 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 161.2+ KB\n"
     ]
    }
   ],
   "source": [
    "converted_category_df = pd.DataFrame(corpus_converted_category, columns=[\"computed_category\"])\n",
    "lda_category_index_df = pd.DataFrame(corpus_lda_category_index, columns=[\"lda_category_index\"])\n",
    "lda_category_topics_df = pd.DataFrame(corpus_lda_category_topics , columns=[\"lda_category_topics\"])\n",
    "new_category_df = pd.concat([converted_category_df, lda_category_index_df, lda_category_topics_df], axis=1)\n",
    "print(new_category_df.shape)\n",
    "new_category_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>computed_category</th>\n",
       "      <th>lda_category_index</th>\n",
       "      <th>lda_category_topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARTS &amp; CULTURE</td>\n",
       "      <td>Modeling Agencies Enabled Sexual Predators For...</td>\n",
       "      <td>In October 2017, Carolyn Kramer received a dis...</td>\n",
       "      <td>WOMEN</td>\n",
       "      <td>6</td>\n",
       "      <td>[abuse, sexual assault, statement, allegations...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ARTS &amp; CULTURE</td>\n",
       "      <td>Actor Jeff Hiller Talks “Bright Colors And Bol...</td>\n",
       "      <td>This week I talked with actor Jeff Hiller abou...</td>\n",
       "      <td>ARTS &amp; CULTURE</td>\n",
       "      <td>16</td>\n",
       "      <td>[film, season, movie, music, book, series, song]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ARTS &amp; CULTURE</td>\n",
       "      <td>New Yorker Cover Puts Trump 'In The Hole' Afte...</td>\n",
       "      <td>The New Yorker is taking on President Donald T...</td>\n",
       "      <td>WOMEN</td>\n",
       "      <td>6</td>\n",
       "      <td>[abuse, sexual assault, statement, allegations...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ARTS &amp; CULTURE</td>\n",
       "      <td>Man Surprises Girlfriend By Drawing Them In Di...</td>\n",
       "      <td>Kellen Hickey, a 26-year-old who lives in Huds...</td>\n",
       "      <td>ARTS &amp; CULTURE</td>\n",
       "      <td>20</td>\n",
       "      <td>[art, artists, artist, space, human, works, pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ARTS &amp; CULTURE</td>\n",
       "      <td>This Artist Gives Renaissance-Style Sculptures...</td>\n",
       "      <td>There’s something about combining the traditio...</td>\n",
       "      <td>ARTS &amp; CULTURE</td>\n",
       "      <td>20</td>\n",
       "      <td>[art, artists, artist, space, human, works, pl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         category                                              title  \\\n",
       "0  ARTS & CULTURE  Modeling Agencies Enabled Sexual Predators For...   \n",
       "1  ARTS & CULTURE  Actor Jeff Hiller Talks “Bright Colors And Bol...   \n",
       "2  ARTS & CULTURE  New Yorker Cover Puts Trump 'In The Hole' Afte...   \n",
       "3  ARTS & CULTURE  Man Surprises Girlfriend By Drawing Them In Di...   \n",
       "4  ARTS & CULTURE  This Artist Gives Renaissance-Style Sculptures...   \n",
       "\n",
       "                                                body computed_category  \\\n",
       "0  In October 2017, Carolyn Kramer received a dis...             WOMEN   \n",
       "1  This week I talked with actor Jeff Hiller abou...    ARTS & CULTURE   \n",
       "2  The New Yorker is taking on President Donald T...             WOMEN   \n",
       "3  Kellen Hickey, a 26-year-old who lives in Huds...    ARTS & CULTURE   \n",
       "4  There’s something about combining the traditio...    ARTS & CULTURE   \n",
       "\n",
       "   lda_category_index                                lda_category_topics  \n",
       "0                   6  [abuse, sexual assault, statement, allegations...  \n",
       "1                  16   [film, season, movie, music, book, series, song]  \n",
       "2                   6  [abuse, sexual assault, statement, allegations...  \n",
       "3                  20  [art, artists, artist, space, human, works, pl...  \n",
       "4                  20  [art, artists, artist, space, human, works, pl...  "
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_df = pd.concat([articles_df,new_category_df],axis=1)\n",
    "category_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37849243306169966"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_df[category_df[\"category\"]==category_df[\"computed_category\"]].shape[0]/(category_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>lda_category_topics</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th>lda_category_index</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">ARTS &amp; CULTURE</th>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">WOMEN</th>\n",
       "      <th>17</th>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>280 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   lda_category_topics\n",
       "category       lda_category_index                     \n",
       "ARTS & CULTURE 0                                     6\n",
       "               1                                    39\n",
       "               2                                     2\n",
       "               4                                     7\n",
       "               5                                     1\n",
       "...                                                ...\n",
       "WOMEN          17                                  110\n",
       "               18                                    9\n",
       "               20                                   11\n",
       "               22                                    1\n",
       "               23                                  106\n",
       "\n",
       "[280 rows x 1 columns]"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_category_df = category_df[[\"category\", \"lda_category_index\", \"lda_category_topics\"]]\n",
    "count_category_df = reduced_category_df.groupby([\"category\", \"lda_category_index\"], sort=True).count()\n",
    "count_category_df[count_category_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lda-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:16:33) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3ad0e796c2702896c2435601fe0d20e0c8f831fc599d2a58cf59deb3de6a6b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
